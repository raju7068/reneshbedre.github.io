---
title: "Hypothesis testing"
date:   2020-11-22 010:19:20
permalink: blog/hypotest.html
author_profile: true
classes: wide
published: false
tags:
  - Statistics
---

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>



## <span style="color:#33a8ff">What is hypothesis testing?</span>
- Hypothesis testing refers to comparing the samples and draw the conclusions based on the appropriate statistical 
  tests. For example, gene expression between two conditions, yield of two plant genotypes, association between drug 
  treatment and patient survival, comparing sample mean with the population mean, effect of multiple fertilizers on 
  plat growth etc.
  
## <span style="color:#33a8ff">Null and alternate hypothesis</span>

- The Hypothesis testing are useful to answer the research questions and should be proposed before the experiment. 
  For example, are the changes in expression of  some genes are induced by the treatment condition? This research 
  question can be stated simply in terms of null hypothesis (H<sub>0</sub>) as "there is no difference in gene 
  expression between control and diseased conditions" versus alternate hypothesis (H<sub>a</sub>) "there is difference 
  in gene expression between control and diseased conditions".
- The appropriate statistical tests are then applied to test the null hypothesis against the alternate hypothesis. For
  above example, two sample <i>t</i>-test would be appropriate to test the gene expression differences between two 
  conditions.
- The statistical tests based on the collected data provide a evidence based on the <i>p</> value to reject or fail 
  to reject the null hypothesis. If the <i>p</> is 0.01, it suggests that there are 1 chance out of 100 that you would 
  obtain the difference in expression of gene between two condition when null hypothesis is true. Generally, the null hypothesis
  is rejected at the 0.05 significance level (&alpha;)

### One- and two-tailed (sided) alternate hypothesis

- One-tailed or one-sided hypothesis specifies the direction of the outcome (either greater of lesser). 
- One-tailed hypothesis are appropriate when only one direction of the outcome is more meaningful 
  (e.g. drug has more side effects than control)
- Two-tailed or two-sided hypothesis would check if there is difference (either greater of lesser) in the expression
  of gene between control and diseased conditions.
- Examples of one- and two-tailed null hypothesis, <br>
   one-tailed (greater) null hypothesis "H<sub>0</sub>: expression of gene is higher in diseased condition than control 
   condition" <br>
   one-tailed (lesser) null hypothesis "H<sub>0</sub>: expression of gene is lesser in diseased condition than control 
   condition" <br>
   two-tailed (greater or lesser) null hypothesis "H<sub>0</sub>: there is difference in gene expression between 
   control and diseased conditions"
   
<p align="center">
<img src="/assets/posts/hyptest/tdist.png" width="800">
</p>


## <span style="color:#33a8ff">Type I (&alpha;), type II errors (&beta;), and power (1-&beta;)</span>

- Now, we have the null and alternate hypothesis, and collected the data for statistical analysis. For gene expression
  example, the two-sample <i>t</i>-test can be conducted to test the null hypothesis against alternate hypothesis. 
- If the <i>p</i> value obtained from <i>t</i>-test is less than the significance level (&alpha;) 0.05 (|<i>t</i>| > 
  t critical) , the null hypothesis is rejected and difference is statistically significant. 
- Here, &alpha; = 0.05 (5%) represents the maximum chance of rejecting the null hypothesis when it is actually true (fail to 
  reject null hypothesis). The significance level (&alpha;) is also know as type I error (false positive). The 5% 
  significance level is arbitrary and can be changed based on the study design and research questions. 
- If the <i>p</i> value is 0.01, it suggests that there is 1 chance out of 100 that you would obtain the difference in 
  expression of gene between two condition when null hypothesis is true.
  
  | null hypothesis | difference  | no difference |
|----|----|----|
| reject | true | type I error (&alpha;)|
| fail to reject | type II error (&beta;) | true | 

- type II error (&beta;) occurs when the null hypothesis is fail to reject when it is actually reject.  The quantity
  1-&beta; is defined as the power (probability of not doing type II error). 
  
## <span style="color:#33a8ff">Hypothesis testing example</span>

- Suppose, we have 50 ball samples collected from the production lines and would like to check if the mean diameter
  of the ball is 5 cm. 
- First, let's propose the one- and two-tailed null hypothesis <br>
   one-tailed (greater) null hypothesis "H<sub>0</sub>: Ball mean diameter is greater than 5 cm  <br>
   one-tailed (lesser) null hypothesis "H<sub>0</sub>: Ball mean diameter is lesser than 5 cm <br>
   two-tailed (greater or lesser) null hypothesis "H<sub>0</sub>: Ball mean diameter is not equal to 5 cm 
- Plot the <i>t</i> distribution with 49 degrees of freedom (see formula for <i>t</i> test and degree of freedom)
  
  
  
```python
# get count plot for the cancer outcome
import seaborn as sns
ax = sns.countplot(x='dign', data=df_train)
plt.show()
```  

<p align="center">
<img src="/assets/posts/logit/countplot.svg" width="500">
</p>

### Feature selection for model training
- For good predictions of the regression outcome, it is essential to include the good independent variables (features) for 
  fitting the regression model (e.g. variables that are not highly correlated). If you include all features, there are 
  chances that you may not get all significant predictors in the model.
- Let's visualize the data for correlation among the independent variables

 ```python
from bioinfokit import visuz
X = df_train.iloc[:,2:12]
visuz.stat.corr_mat(df=X, cmap='RdBu')
```
<p align="center">
<img src="/assets/posts/logit/corr_mat.svg" width="600">
</p>

- As you see in the correlation figure, several variables are highly correlated (multicollinearity) to each other 
  (e.g. rad_mean and peri_mean). Multicollinearity can be an issue and reduce the performance of the fitted model. These 
  spurious variables can be detected and dropped using various methods such the 
  <a href='https://reneshbedre.github.io/blog/mlr.html#interpretation'>multicollinearity (VIF)</a>, dimension reduction 
  by <a href='https://reneshbedre.github.io//blog/pca_3d.html'>PCA</a>, recursive feature elimination (RFE), fitting 
  models with all variables and removing insignificant variables, Chi-squared test etc.
- I have used the model fitting and variance inflation factor (VIF) to drop the features with high multicollinearity and
  insignificant variables. 
- Based on model fitting and VIF analysis, I am using only text_mean, peri_mean, smooth_mean, conv_mean, and frac_dim_mean for the 
  logistic regression analysis.
  


### Logistic regression model fitting
```python
import numpy as np
import matplotlib.pyplot as plt
plt.subplots(figsize=(4,4))
x=np.arange(-5,5,0.01)
xt=np.arange(-5,6,2)
y=stats.t.pdf(x,10)
ylim=np.arange(0,0.55, 0.1)
# critical value for one tailed at df=10 and alpha=0.05 1.812
s=np.arange(-5,-1.812,0.01)
sy=stats.t.pdf(s,10)
plt.plot(x,y)
plt.fill_between(s,sy, color='#f48d60', label='Rejection region')
plt.xlabel('t value')
plt.ylabel('Probability density')
plt.xticks(xt)
plt.yticks(ylim)
plt.legend(loc='upper left')
plt.savefig('tdist.svg', format='svg', bbox_inches='tight', dpi=600)
plt.close()

plt.subplots(figsize=(4,4))
x=np.arange(-5,5,0.01)
xt=np.arange(-5,6,2)
y=stats.t.pdf(x,10)
ylim=np.arange(0,0.55, 0.1)
# critical value for one tailed at df=10 and alpha=0.05 1.812
s=np.arange(1.812, 5, 0.01)
sy=stats.t.pdf(s,10)
plt.plot(x,y)
plt.fill_between(s,sy, color='#f48d60', label='Rejection region')
plt.xlabel('t value')
plt.ylabel('Probability density')
plt.xticks(xt)
plt.yticks(ylim)
plt.legend(loc='upper left')
plt.savefig('tdistgreater.svg', format='svg', bbox_inches='tight', dpi=600)
plt.close()

plt.subplots(figsize=(4,4))
x=np.arange(-5,5,0.01)
xt=np.arange(-5,6,2)
y=stats.t.pdf(x,10)
ylim=np.arange(0,0.55, 0.1)
# critical value for one tailed at df=10 and alpha=0.05 2.228
s1=np.arange(2.228, 5, 0.01)
s2=np.arange(-5, -2.228, 0.01)
sy1=stats.t.pdf(s1,10)
sy2=stats.t.pdf(s2,10)
plt.plot(x,y)
plt.fill_between(s1,sy1, color='#f48d60', label='Rejection region')
plt.fill_between(s2,sy2, color='#f48d60')
plt.xlabel('t value')
plt.ylabel('Probability density')
plt.xticks(xt)
plt.yticks(ylim)
plt.legend(loc='upper left')
plt.savefig('tdisttwo.svg', format='svg', bbox_inches='tight', dpi=600)
plt.close()

```

  

<div class="AW-Form-991894911"></div>
<script type="text/javascript">(function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//forms.aweber.com/form/11/991894911.js";
    fjs.parentNode.insertBefore(js, fjs);
    }(document, "script", "aweber-wjs-mlg57bayy"));
</script>

<form action="{{site.mailchimp-list}}" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
    <div class="mc-field-group">
        <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on">
        <input type="submit" value="Subscribe" name="subscribe" class="heart">
    </div>
</form>

<style>
    .wj-contact-form input {
        vertical-align: middle;
        margin-top: 0.25em;
        margin-bottom: 0.5em;
        padding: 0.75em;
        font-family: monospace, sans-serif;
        border:1px solid #444;
        outline-color: #2e83e6;
        border-radius: 3px;
        transition: box-shadow .2s ease;
    }
    
    .wj-contact-form input[type="submit"] {
        background-color: #2e83e6;
        border: 1px solid #2e83e6;;
        color: #eee;
    }
</style> 


## <span style="color:#33a8ff">References</span>
- Banerjee A, Chitnis UB, Jadhav SL, Bhawalkar JS, Chaudhury S. Hypothesis testing, type I and type II errors. Industrial psychiatry journal. 2009 Jul;18(2):127.
https://link.springer.com/article/10.1186/cc1493


<p>
{% include  share.html %}
</p>
    
<span style="color:#9e9696"><i> Last updated: November 24, 2020</i> </span>    

<p>
{% include  license.html %}
</p>