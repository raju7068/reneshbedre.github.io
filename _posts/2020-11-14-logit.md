---
title: "Logistic regression, feature selection, and classification"
date:   2020-11-13 010:19:20
permalink: blog/logit.html
author_profile: true
classes: wide
published: false
tags:
  - Machine learning
  - Statistics
  - Classification
---

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>


## <span style="color:#33a8ff">What is logistic regression?</span>
- Logistic regression (logit model) models the binary (dichotomous) response variable (e.g. 0 and 1, true and false) 
  as linear combinations of the single or multiple independent (also called predictor or explanatory) variables. 
- Univariate logistic regression has one independent variable, and multivariate logistic regression has  more than one 
  independent variables.
- In logistic regression, the probability or odds of the response variable (instead of values as in linear regression)
  are modeled as function of the independent variables.
- For example, prediction of death or survival of patients, which can be coded as 0 and 1, can be predicted by 
  metabolic markers.

## <span style="color:#33a8ff">Logistic regression model</span>

<p align="center">
  \( ln\frac{\it{p}(y)}{1-\it{p}(y)} = ln(odds) = a + b_1x_1 + b_2x_2 + b_3x_3 + ... + b_nx_n  \) 
  </p>
  
  In terms of <em>p</em>
  <p align="center">
  \( \it{p} = \frac{exp(a + b_1x_1 + b_2x_2 + b_3x_3 + ... + b_nx_n)}{1+exp(a + b_1x_1 + b_2x_2 + b_3x_3 + ... + b_nx_n)}  \) 
  </p>
  
  <p>
  Where, \(  ln[ \frac{\it{p}(y)}{1-\it{p}(y)}] \)  = natural log of odds,<br>
          \(  \it{p}  \) = probability of event, <br> 
         \( x_1, x_2, ..., x_n \) = independent variables <br>
         a = y-intercept, <br> \( b_1, b_2, ..., b_n \) = slope of the regression <br>
         Intercept and slopes are also called coefficients of regression
  </p>
  
  <p> Logistic regression model follow binomial distribution and the coefficients of regression (parameter estimates)
    are estimated using the maximum likelihood estimation (MLE). The Logistic regression model the output as the odds 
    which assign the probability to the observations for classification. </p>

## <span style="color:#33a8ff">Odds and Odds ratio (OR)</span>
- Odds is the ratio of the probability of event happening to the probability of event not happening 
  (<i>p</i> &#8725; 1-<i>p</i>). Odds can range from 0 to +&#8734;.
- Odds ratio (OR) is the ratio of two odds. OR can range from 0 to +&#8734;. OR is important in interpreting the 
  coefficients of regressions i.e effect of independent variables on the response variable, as coefficients of 
  regressions would not be easy to interpret. OR can be obtained by exponentiating the coefficients of regressions.
  

## <span style="color:#33a8ff">Logistic regression in python</span>
- We will use sklearn and bioinfokit (v1.0.4 or later)

```python
# I am using interactive python interpreter (Python 3.7.4)
from bioinfokit.analys import get_data
# get dataset for model training
df_train = get_data('wdbc_train').data
df_train.head()
        ID  dign  rad_mean  text_mean  peri_mean  area_mean  smooth_mean  comp_mean  conv_mean  conv_p_mean  sym_mean  frac_dim_mean
0  8711202     1     17.68      20.74     117.40      963.7      0.11150    0.16650    0.18550      0.10540    0.1971        0.06166
1   869218     0     11.43      17.31      73.66      398.0      0.10920    0.09486    0.02031      0.01861    0.1645        0.06562
2   899667     1     15.75      19.22     107.10      758.6      0.12430    0.23640    0.29140      0.12420    0.2375        0.07603
3   917062     0     12.88      18.22      84.45      493.1      0.12180    0.16610    0.04825      0.05303    0.1709        0.07253
4  8912284     0     12.89      15.70      84.08      516.6      0.07818    0.09580    0.11150      0.03390    0.1432        0.05935

# get test dataset
df_test = get_data('wdbc_test').data
```

- This dataset represents the characteristics of breast cancer cell nuclei computed from the digitized images 
  (Dua and Graff 2019; Dr. William H. Wolberg, University Of Wisconsin Hospital at Madison). 
- The features calculated from the digitized cell images includes, radius, texture, perimeter, area, smoothness, 
  compactness, concavity, concave points, symmetry, and fractal dimension for mean , standard error, and largest 
  (worst) values. We have only used mean values of these features (continuous variables) for regression analysis.
- The outcome (response variable) measured as malignant (1) or benign (0) (see dign variable in dataframe)
- Using logistic regression model, I will build a classifier to predict the the outcome as malignant or benign from given test 
  samples
  
Data distribution for binary outcome variable,
```python
# get count plot for the cancer outcome
import seaborn as sns
ax = sns.countplot(x='dign', data=df_train)
plt.show()
```  

<p align="center">
<img src="/assets/posts/logit/countplot.svg" width="500">
</p>

### Feature selection
- For good predictions of the regression outcome, it is essential to include the good independent variables (features) for 
  fitting the regression model (e.g. variables which are not highly correlated). If you include all features, there are 
  high chances that you may not get all significant predictors in the model.
- Let's visualize the data for correlation among the independent variables
 ```python
from bioinfokit import visuz
X = df_train.iloc[:,2:12]
visuz.stat.corr_mat(df=X, cmap='RdBu')
```
<p align="center">
<img src="/assets/posts/logit/corr_mat.svg" width="600">
</p>

- As you see in the correlation figure, several variables are highly correlated to each other 
  (e.g. rad_mean and peri_mean).  Several techniques can be used to identify the good features based on the  
  multicollinearity, dimnesion reduction by PCA, recursive feature elimination (RFE), fitting models with all variables and 
  removing insignificant variables etc.
- I have used the variance inflation factor (VIF) which helps to detect multicollinearity to drop the features with 
  very high VIF (Read more about VIF here)
- Based on VIF analysis, I am using only text_mean, smooth_mean, sym_mean, and frac_dim_mean for the logistic regression'
  analysis (variables with lowest VIF)
  


### Logistic regression model fitting
```python
# logistic regression model
import statsmodels.api as sm 
# get independent variables
X = df_train[['text_mean', 'smooth_mean', 'sym_mean', 'frac_dim_mean']]
# to get intercept -- this is optional
# X = sm.add_constant(X)
# get response variables
Y = df_train[['dign']]
# fit the model with maximum likelihood function
model = sm.Logit(endog=Y, exog=X).fit()
# output message
Optimization terminated successfully.
         Current function value: 0.456777
         Iterations 7

print(model.summary())
# output
                            Logit Regression Results                           
==============================================================================
Dep. Variable:                   dign   No. Observations:                  426
Model:                          Logit   Df Residuals:                      422
Method:                           MLE   Df Model:                            3
Date:                Sun, 22 Nov 2020   Pseudo R-squ.:                  0.3033
Time:                        14:02:52   Log-Likelihood:                -194.59
converged:                       True   LL-Null:                       -279.29
Covariance Type:            nonrobust   LLR p-value:                 1.716e-36
=================================================================================
                    coef    std err          z      P>|z|      [0.025      0.975]
---------------------------------------------------------------------------------
text_mean         0.1785      0.029      6.195      0.000       0.122       0.235
smooth_mean     111.5813     15.615      7.146      0.000      80.976     142.186
sym_mean         21.2923      5.836      3.648      0.000       9.853      32.732
frac_dim_mean  -302.8844     31.433     -9.636      0.000    -364.492    -241.277
=================================================================================

# get odds ratio
np.exp(model.params)
# output
text_mean         1.195387e+00
smooth_mean       2.878313e+48
sym_mean          1.766610e+09
frac_dim_mean    2.877296e-132
    
```

### Interpretation
Fitted model,

<p align="center">
  \( ln\frac{\it{p}(y)}{1-\it{p}(y)} = -12.16 + 0.32*\text{text_mean} + 135.38*\text{smooth_mean}  + 30.38*\text{sym_mean} - 221.71*\text{frac_dim_mean}  \) 
  </p>

- The variable text_mean has OR of 1.19 which suggests for one unit increase in text_mean we expect that about 1.19 times 
  increase the odds of patient being malignant (assuming all other independent variables constant). Other independent 
  variables can be interpreted in same way.
- The <i>p</i> values for all independent variables are significant (<i>p</i> < 0.05) and suggests that these variables 
  are highly associated with the outcome
- Fractal dimension have slight effect on cancer classification due to its very low OR
- The fitted model can be evaluated using the goodness-of-fit index pseudo R-squared (McFaddenâ€™s R2 index) which 
  measures improvement in model likelihood over the null model (unlike OLS R-squared which measures proportion of 
  explained variance --check here). The pseudo R-squared value close to 1 suggest better fitted model. However, it should
  be interpreted cautiously and other measures should also be considered for model evaluation. pseudo R-squared would
  be more useful when comparing the different models for the similar datasets predicting the same outcome. 
  
  The multicollinearity among the independent variables can reduce the value of pseudo R-squared.

### Prediction of test dataset using fitted model

```python
# get the predicted values for the test dataset [0, 1]
pred = model.predict(exog=df_test[['text_mean', 'smooth_mean', 'sym_mean', 'frac_dim_mean']])
pred.head()
# output
0    0.341360
1    0.223005
2    0.896689
3    0.553766
4    0.016013

# predicted values > 0.5 classified as malignant (1) and <= 0.05 as benign (0)
round(pred)
0      0.0
1      0.0
2      1.0
3      1.0
4      0.0

# get confusion matrix and accuracy of the prediction
from sklearn.metrics import accuracy_score, confusion_matrix
confusion_matrix(y_true=list(df_test['dign']), y_pred=list(round(pred)))
# output
array([[75, 11],
       [27, 30]])

# fitted model accuracy
accuracy_score(y_true=list(df_test['dign']), y_pred=list(round(pred)))
# output
0.7342
```
Confusion matrix,

| Predicted <br> Observed  | M(1)  | B(0) |
|----|----|----|
| M(1) | 75 | 11 |
| B(0) | 27 | 30 | 

In confusion matrix diagonal numbers (75 and 30) indicates the correct predictions [true positives (TP) and true 
negatives (TN)] for the malignant (1) and benign (0) outcomes for test cancer datasets. The other numbers (11 and 27) 
indicates incorrect predictions [false negatives (FN) and false positives (FP)]

<p align="center">
  \( \text{Sensitivity or True positive rate (TPR) } = \frac{TP}{TP+FN} = \frac{75}{75+11} = 0.8720 \) 
  <br><br><br>
  \( Specificity = \frac{TN}{TN+FP} = \frac{30}{30+27} = 0.5263 \) = 1 - FPR
  <br><br><br>
  \( Precision = \frac{TP}{TP+FP} = \frac{75}{75+27} = 0.7352 \)
  <br><br><br>
  \( Recall = \frac{TP}{TP+FN} = \frac{75}{75+11} = 0.8720 \)
  <br><br><br>
  \( \text{False positive rate (FPR)} = \frac{FP}{FP+TN} = \frac{27}{27+30} = 0.4736 = 1 - Specificity\)
  <br><br><br>
  </p>
  
Plot Receiver Operating Characteristic (ROC) curve,

```python
from sklearn.metrics import roc_curve, auc, roc_auc_score
from bioinfokit.visuz.stat import roc

fpr, tpr, thresholds = roc_curve(y_true=list(df_test['dign']), y_score=list(pred))
auc = roc_auc_score(y_true=list(df_test['dign']), y_score=list(pred))
# plot ROC
roc(fpr=fpr, tpr=tpr, auc=auc, shade_auc=True, per_class=True, legendpos='upper center', legendanchor=(0.5, 1.1))
```

<p align="center">
<img src="/assets/posts/logit/roc.svg" width="600">
</p>

#### Interpretation
- In ROC, we can summarize the model predictability based on the area under curve (AUC). AUC range from 0.5 to 1 and
  model with higher the AUC has higher predictability. AUC refers to the probability that randomly chosen malignant 
  patients will have high chances of classification as malignant than randomly chosen benign patients.
- The fitted model has AUC 0.8315 suggesting the better predictability in patient classification for breast cancer. 
  The points lying above the chance level and close to grey line (perfect performance) represents a model with higher 
  higher predictability.
- The accuracy of the  fitted model is 0.7342. Even though accuracy is a measure of model performance, it is not
  alone enough. The AUC outperforms accuracy for model predictability. Two models can have same accuracy but can 
  differ in AUC. The models which are evaluated solely on accuracy could lead to misleading classification.


https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1065119/
https://journals.lww.com/ajpmr/fulltext/2000/11000/logistic_regression__a_nontechnical_review.17.aspx
https://www.scirp.org/journal/paperinformation.aspx?paperid=95655
https://christophm.github.io/interpretable-ml-book/logistic.html
https://www.unm.edu/~schrader/biostat/bio2/Spr06/lec11.pdf
https://rpubs.com/OmaymaS/182726
https://rpubs.com/hngu785/624087
https://ieeexplore.ieee.org/document/7361138 good
https://www.ajronline.org/doi/10.2214/AJR.07.3345
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3755824/ for ROC

## <span style="color:#33a8ff">References</span>
- Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.
- Dr. William H. Wolberg, General Surgery Dept. University of Wisconsin, Clinical Sciences Center Madison, WI 53792
- Bewick V, Cheek L, Ball J. Statistics review 14: Logistic regression. Critical care. 2005 Feb 1;9(1):112.
- Hanley JA, McNeil BJ. The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology. 1982 Apr;143(1):29-36.
- Smith TJ, McKenna CM. A comparison of logistic regression pseudo R2 indices. Multiple Linear Regression Viewpoints. 2013;39(2):17-26.
- Abdulhafedh A. Incorporating the multinomial logistic regression in vehicle crash severity modeling: a detailed overview. Journal of Transportation Technologies. 2017;7(03):279.
